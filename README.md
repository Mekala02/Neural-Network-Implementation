# Neural-Network-Implementation
 This is my implementation of neural network.

Features:
You can easyly add layers to network.
You can save or load your trained network.


Activation Functions:
    Sigmoid
    ReLu
    Softmax

Regularization:
    L2
    Dropout (You can individually drop hidden layers)

Optimizers:
    RMS
    Adam

Weight initilazition (Kernel Initializer):
    He
    Xavier
    (You can choose kernel for individual layers)